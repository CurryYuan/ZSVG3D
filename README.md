
<p align="center">
  <h1 align="center">Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding</h1>

  <p align="center">
    Zhihao Yuan, Jinke Ren, Chun-Mei Feng, Hengshuang Zhao, Shuguang Cui, Zhen Li
  </p>

  <h2 align="center">CVPR 2024</h2>


<p align="center">
    <a href='https://arxiv.org/abs/2311.15383'>
      <img src='https://img.shields.io/badge/Paper-PDF-red?style=flat&logo=arXiv&logoColor=red' alt='Paper PDF'>
    </a>
    <a href='https://curryyuan.github.io/ZSVG3D/' style='padding-left: 0.5rem;'>
      <img src='https://img.shields.io/badge/Project-Page-blue?style=flat&logo=Google%20chrome&logoColor=blue' alt='Project Page'>
    </a>
  </p>

</p>




<!-- <img src="docs/static/images/figure_1.png" width="75%"> -->

  <p align="center">
    <img src="docs/static/images/figure_1.png" alt="Logo" width="80%">
  </p>

*Comparative overview of two 3DVG approaches.*
(a) Supervised 3DVG involves input from 3D scans combined with text queries,
guided by
object-text pair annotations, (b) Zero-shot 3DVG identifies the location of target
objects using
programmatic representation generated by LLMs, i.e., target category, anchor
category, and
relation grounding, thereby highlighting its superiority in decoding spatial relations and
object identifiers
within a given space, e.g., the location of the keyboard (outlined in green) can be
retrieved based on
the distance between the keyboard and the door (outlined in blue).


## Instructions

### Visual Programming Generation
Run the following command. You need to modify the OpenAI key first.
```python
python gen_visprog.py
```

### Zero-shot evaluation
Download our preproceed 3D features from [here](https://cuhko365-my.sharepoint.com/:u:/g/personal/221019046_link_cuhk_edu_cn/ERMP88uTVCNLhzofKub7MsMBvaRAFXVr5abbQUjRYyYDiA?e=x6aKC9) and place them under `data/scannet` folder.

Run the following command:
```python
python visprog_nr3d.py
```

Uncomment this line to use the BLIP2 models for LOC module. You can download our preprocessed images from [here](https://cuhko365-my.sharepoint.com/:u:/g/personal/221019046_link_cuhk_edu_cn/Ed4HCYSQh5xDgmaCM4PatOsBWHpri34gHXePO2VwUKJWfw?e=XTYz8Z) and change the [image_path](https://github.com/CurryYuan/ZSVG3D/blob/11c67346215bbda1c01a136bb75403667eac13e2/zsvg/loc_interpreters.py#L114) to your downloaded path.



### Data Preparation

You can also process the features by yourself.

First, install the dependencies: 
```bash
cd ./models/pointnext/PointNeXt
bash install.sh
```

Prepare ScanNet 2D data following [OpenScene](https://github.com/pengsongyou/openscene/blob/main/scripts/preprocess/README.md) and 3D data following [vil3dref](https://github.com/cshizhe/vil3dref).

<!-- Download our pretrained classifier from [here](https://drive.google.com/file/d/1Q6Q6) which is fine-tuned from [ULIP](https://github.com/salesforce/ULIP), and place it under `weights` folder. -->

Then, run the following scripts:
```python
python preprocess/process_feat_3d.py
python preprocess/process_feat_2d.py
```

You can refer to `preprocess/process_mask3d.ipynb` for processing 3D instance segments.
